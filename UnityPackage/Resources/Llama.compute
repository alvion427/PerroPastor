#pragma multi_compile QUANT_WEIGHT_32 QUANT_WEIGHT_16 QUANT_WEIGHT_Q5_1 QUANT_WEIGHT_Q8_0
//#define QUANT_WEIGHT_Q5_1 1
#pragma multi_compile QUANT_RUNTIME_32 QUANT_RUNTIME_16

#include "Common.cginc"

#if QUANT_WEIGHT_16
    #define WEIGHT_BLOCK_TYPE uint2
    #define STORE_WEIGHT StoreVec16f
    #define LOAD_WEIGHT LoadVec16f
    #define WEIGHT_IS_QUANTIZED 0
#elif QUANT_WEIGHT_Q5_1
    #define WEIGHT_BLOCK_VEC 8
    #define WEIGHT_BLOCK_TYPE Q5_1_Block
    #define ENCODE_WEIGHT Encode_Q5_1
    #define DECODE_WEIGHT Decode_Q5_1
    #define WEIGHT_IS_QUANTIZED 1
#elif QUANT_WEIGHT_Q8_0
    #define WEIGHT_BLOCK_VEC 8
    #define WEIGHT_BLOCK_TYPE Q8_0_Block
    #define ENCODE_WEIGHT Encode_Q8_0
    #define DECODE_WEIGHT Decode_Q8_0
    #define WEIGHT_IS_QUANTIZED 1
#else /* QUANT_WEIGHT_32 */
    #define WEIGHT_BLOCK_TYPE float4
    #define STORE_WEIGHT StoreVec32f
    #define LOAD_WEIGHT LoadVec32f
    #define WEIGHT_IS_QUANTIZED 0
#endif

#if QUANT_RUNTIME_16
    #define RUNTIME_TYPE_VEC uint2
    #define StoreRuntime StoreVec16f
    #define LoadRuntime LoadVec16f
#else  /* QUANT_RUNTIME_32 */
    #define RUNTIME_TYPE_VEC float4
    #define StoreRuntime StoreVec32f
    #define LoadRuntime LoadVec32f
#endif

/// Clear
#pragma kernel Clear

RWStructuredBuffer<float> clear_dest;
uint clear_length;

[numthreads(256, 1, 1)]
void Clear(uint3 id : SV_DispatchThreadID) {
    if (id.x >= clear_length) return;
    clear_dest[id.x] = 0;
}

/// Memcpy
#pragma kernel Memcpy

RWStructuredBuffer<RUNTIME_TYPE_VEC> copy_dest;
StructuredBuffer<float4> copy_source;
uint memcpy_veclen;
uint memcpy_source_offset;
uint memcpy_dest_offset;

[numthreads(256, 1, 1)]
void Memcpy(uint3 id : SV_DispatchThreadID)
{
    // Check if index is within bounds
    if (id.x < memcpy_veclen)
    {
        // TODO: Change the load to also use LoadRuntime() once we convert everything to float16
        float4 v = copy_source[id.x + memcpy_source_offset];
        copy_dest[id.x + memcpy_dest_offset] = StoreRuntime(v);
    }
}

/// ScaleBuffer
#pragma kernel ScaleBuffer

RWStructuredBuffer<float4> scalebuffer_buffer;
uint scalebuffer_veclen;
float scalebuffer_scale;

[numthreads(kThreadsPerGroup, 1, 1)]
void ScaleBuffer(uint3 id : SV_DispatchThreadID) {
    if (id.x >= scalebuffer_veclen) return;
    
    scalebuffer_buffer[id.x] = scalebuffer_buffer[id.x] * scalebuffer_scale;
}


/// Fixed to float
#pragma kernel FixedToFloat

RWStructuredBuffer<float4> fixedtofloat_dest;
StructuredBuffer<int4> fixedtofloat_source;
uint fixedtofloat_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void FixedToFloat(uint3 id : SV_DispatchThreadID) {
    if (id.x >= fixedtofloat_length) return;
    fixedtofloat_dest[id.x] = fixedtofloat_source[id.x] / kFixedPointScale;
}

#pragma kernel LoadEmbedding

StructuredBuffer<int> loadembedding_token;
StructuredBuffer<WEIGHT_BLOCK_TYPE> loadembedding_source;
RWStructuredBuffer<float4> loadembedding_dest;
uint loadembedding_blockCount;

[numthreads(kThreadsPerGroup, 1, 1)]
void LoadEmbedding(uint3 id : SV_DispatchThreadID) {
    if (id.x >= loadembedding_blockCount) return;

    int token = loadembedding_token[0];
    int sourceIndex = token * loadembedding_blockCount + id.x;

#if !WEIGHT_IS_QUANTIZED    
    loadembedding_dest[id.x] = LOAD_WEIGHT(loadembedding_source[sourceIndex]);
#else
    const WEIGHT_BLOCK_TYPE block = loadembedding_source[sourceIndex];

    int destOffset = id.x * WEIGHT_BLOCK_VEC;

    [unroll]
    for (uint valueIndex = 0; valueIndex < WEIGHT_BLOCK_VEC; ++valueIndex)
    {
        float4 v = DECODE_WEIGHT(block, valueIndex);
        loadembedding_dest[destOffset + valueIndex] = v;
    }
#endif
}

/// MatMul
#pragma kernel MatMul

StructuredBuffer<WEIGHT_BLOCK_TYPE> matmul_matrixW;
StructuredBuffer<float4> matmul_vectorX;
RWStructuredBuffer<float> matmul_vectorOut;
uint matmul_rows;
uint matmul_blocksPerRow;

[numthreads(kThreadsPerGroup, 1, 1)]
void MatMul(uint3 id : SV_DispatchThreadID) {
    uint row = id.x;
    if (row >= matmul_rows) return;
    
    float sum = 0;
#if !WEIGHT_IS_QUANTIZED    
    for (uint col = 0; col < matmul_blocksPerRow; col++) {
        float4 a = LOAD_WEIGHT(matmul_matrixW[row * matmul_blocksPerRow + col]);
        float4 b = matmul_vectorX[col];
        sum += dot(a, b);
    }
#else
    for (uint b = 0; b < matmul_blocksPerRow; ++b)
    {
        const WEIGHT_BLOCK_TYPE block = matmul_matrixW[row * matmul_blocksPerRow + b];
        
        [unroll]
        for (uint valueIndex = 0; valueIndex < WEIGHT_BLOCK_VEC; ++valueIndex)
        {
            uint col = b * WEIGHT_BLOCK_VEC + valueIndex;
            float4 a = DECODE_WEIGHT(block, valueIndex);
            float4 b = matmul_vectorX[col];
            sum += dot(a, b);
        }
    }

#endif

    matmul_vectorOut[row] = sum;
}

/// Accumulate
#pragma kernel Accumulate

RWStructuredBuffer<float4> accumulate_A;
StructuredBuffer<float4> accumulate_B;
uint accumulate_veclen;

[numthreads(kThreadsPerGroup, 1, 1)]
void Accumulate(uint3 id : SV_DispatchThreadID) {
    if (id.x >= accumulate_veclen) return;
    accumulate_A[id.x] += accumulate_B[id.x];
}

/// RMSNorm
#pragma kernel RMSNorm

StructuredBuffer<float4> rmsnorm_In; // Input values
StructuredBuffer<WEIGHT_BLOCK_TYPE> rmsnorm_Weight; // Weights
RWStructuredBuffer<float4> rmsnorm_Out; // Output values
uint rmsnorm_vecLen;
uint rmsnorm_blockCount;
float rmsnorm_length;

[numthreads(1, 1, 1)]
void RMSNorm(uint3 id : SV_DispatchThreadID) {
    const float kEpsilon = 1e-5f;

    uint j;
    
    // Calculate sum of squares
    float ss = 0.0f;
    for (j = 0; j < rmsnorm_vecLen; j++) {
        ss += dot(rmsnorm_In[j], rmsnorm_In[j]);
    }
    ss /= rmsnorm_length;
    ss += kEpsilon;
    ss = 1.0f / sqrt(ss);

    // Normalize and scale
#if !WEIGHT_IS_QUANTIZED
    for (j = 0; j < rmsnorm_vecLen; ++j)
    {
        float4 weights = LOAD_WEIGHT(rmsnorm_Weight[j]);
        rmsnorm_Out[j] = weights * (ss * rmsnorm_In[j]);
    }
#else
    for (uint b = 0; b < rmsnorm_blockCount; ++b)
    {
        const WEIGHT_BLOCK_TYPE block = rmsnorm_Weight[b];
        for (uint valueIndex = 0; valueIndex < WEIGHT_BLOCK_VEC; ++valueIndex)
        {
            float4 weights = DECODE_WEIGHT(block, valueIndex);
            j = b * WEIGHT_BLOCK_VEC + valueIndex;
            rmsnorm_Out[j] = weights * (ss * rmsnorm_In[j]);
        }
    }
#endif
}

/// Rope
#pragma kernel Rope

RWStructuredBuffer<float2> rope_q; // Query vectors
RWStructuredBuffer<float2> rope_k; // Key vectors
uint rope_head_size;
uint rope_pos;
uint rope_length; // Total length of the vectors

[numthreads(kThreadsPerGroup, 1, 1)]
void Rope(uint3 id : SV_DispatchThreadID) {
    uint idx = id.x;
    if (idx >= rope_length) return;

    uint i = idx * 2;
    uint head_dim = i % rope_head_size;

    float freq = 1.0f / pow(10000.0f, head_dim / (float)rope_head_size);
    float val = rope_pos * freq;
    float fcr = cos(val);
    float fci = sin(val);
    float2 fc = float2(fcr, fci);
    
    float q0 = rope_q[idx].x;
    float q1 = rope_q[idx].y;
    float k0 = rope_k[idx].x;
    float k1 = rope_k[idx].y;

    rope_q[idx].x = q0 * fc.x - q1 * fc.y;
    rope_q[idx].y = q0 * fc.y + q1 * fc.x;
    rope_k[idx].x = k0 * fc.x - k1 * fc.y;
    rope_k[idx].y = k0 * fc.y + k1 * fc.x;
}

#pragma kernel SoftmaxExp
#pragma kernel SoftmaxDivide

StructuredBuffer<float> softmax_input;
RWStructuredBuffer<float> softmax_output;
StructuredBuffer<int> softmax_max_fixed;
RWStructuredBuffer<int> softmax_sum_fixed;
uint softmax_length;
uint softmax_numBatches;
uint softmax_offset;

#define SOFTMAX_STRIDE 8

groupshared int softmax_groupSumFixed;
groupshared float softmax_maxValue;

[numthreads(256, 1, 1)]
void SoftmaxExp(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    if (gid.x == 0)
    {
        softmax_groupSumFixed = 0;
        softmax_maxValue = (softmax_max_fixed[0] / kFixedPointScale);

        // HACK!!! See comment in FindMaxValue
        softmax_maxValue -= 256.0f;
    }

    GroupMemoryBarrierWithGroupSync();
    
    if (id.x < softmax_numBatches)
    {
        uint start = id.x * SOFTMAX_STRIDE;
        uint end = min(start + SOFTMAX_STRIDE, softmax_length); 
        float totalExp = 0;
        for (uint j = start; j < end; ++j)
        {
            float xExp = exp(softmax_input[softmax_offset + j] - softmax_maxValue);
            softmax_output[softmax_offset + j] = xExp;
            totalExp += xExp;
        }

        int xExpFixed = (int)(totalExp * kFixedPointScale);
        InterlockedAdd(softmax_groupSumFixed, xExpFixed);
    }

    GroupMemoryBarrierWithGroupSync();

    if (gid.x == 0)
    {
        InterlockedAdd(softmax_sum_fixed[0], softmax_groupSumFixed);
    }
}

[numthreads(kThreadsPerGroup, 1, 1)]
void SoftmaxDivide(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint i = id.x;
    if (i >= softmax_length) return;

    float sum = softmax_sum_fixed[0] / kFixedPointScale;
    softmax_output[softmax_offset + i] = softmax_input[softmax_offset + i] / sum;
}


/// Compute attention
#pragma kernel ComputeAttention

StructuredBuffer<float4> compute_attention_q; // Query vectors
StructuredBuffer<RUNTIME_TYPE_VEC> compute_attention_k; // Key cache
RWStructuredBuffer<float> compute_attention_att; // Attention scores

uint compute_attention_head; // Which head we are processing
uint compute_attention_head_size_vec; // Size of each head
uint compute_attention_pos; // Current position in the sequence
uint compute_attention_dim_vec; // Dimension of embeddings
uint compute_attention_seq_len; // Length of the sequence

float compute_attention_head_size_inv_sqrt; // 1.0 / Square root of the head size

[numthreads(kThreadsPerGroup, 1, 1)]
void ComputeAttention(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint t = id.x;
    uint att_offset = compute_attention_head * compute_attention_seq_len;
    
    if (t > compute_attention_pos) {
        return;
    }

    uint headOffset = compute_attention_head * compute_attention_head_size_vec;
    uint k_offset = t * compute_attention_dim_vec + headOffset;

    // Compute attention score as dot product of q and k
    float score = 0;
    for (uint i = 0; i < compute_attention_head_size_vec; i++)
    {
        float4 q = compute_attention_q[headOffset + i];
        float4 k = LoadRuntime(compute_attention_k[k_offset + i]);
        score += dot(q, k);
    }
    
    score *= compute_attention_head_size_inv_sqrt;
    compute_attention_att[att_offset + t] = score;
}

/// Silu
#pragma kernel Silu

RWStructuredBuffer<float> silu_InOut;
uint silu_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void Silu(uint3 id : SV_DispatchThreadID) {
    if (id.x >= silu_length) return;
    float x = silu_InOut[id.x];
    silu_InOut[id.x] = x * (1.0f / (1.0f + exp(-x)));
}

/// Elementwise Multiply
#pragma kernel Multiply

RWStructuredBuffer<float> multiply_A;
StructuredBuffer<float> multiply_B;
uint multiply_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void Multiply(uint3 id : SV_DispatchThreadID) {
    if (id.x >= multiply_length) return;
    multiply_A[id.x] *= multiply_B[id.x];
}

/// WeightedSum
#pragma kernel WeightedSum

// Global buffers and variables for Weighted Sum computation
StructuredBuffer<RUNTIME_TYPE_VEC> weightedsum_values; // Value vectors
StructuredBuffer<float> weightedsum_attention; // Attention weights
RWStructuredBuffer<int> weightedsum_out; // Output buffer to hold weighted sum

uint weightedsum_offset_vec; // Offset into value buffer
uint weightedsum_attention_offset; // Offset into attention buffer
uint weightedsum_head_size_vec;
uint weightedsum_pos;
uint weightedsum_dim_vec;

[numthreads(kThreadsPerGroup, 1, 1)]
void WeightedSum(uint3 id : SV_DispatchThreadID) {
    uint t = id.x;
    if (t > weightedsum_pos) return;

    uint valuesOffset = weightedsum_offset_vec + t * weightedsum_dim_vec;

    // accumulate the weighted value into output
    float a = weightedsum_attention[weightedsum_attention_offset + t];
    for (uint i = 0; i < weightedsum_head_size_vec; i++) {
        float4 values = LoadRuntime(weightedsum_values[valuesOffset + i]);
        float4 weightedValues = a * values;
        int4 fixedWeightedValues = (int4)(weightedValues * kFixedPointScale);

        int outputOffset = (weightedsum_offset_vec + i) * 4;
        InterlockedAdd(weightedsum_out[outputOffset + 0], fixedWeightedValues.x);
        InterlockedAdd(weightedsum_out[outputOffset + 1], fixedWeightedValues.y);
        InterlockedAdd(weightedsum_out[outputOffset + 2], fixedWeightedValues.z);
        InterlockedAdd(weightedsum_out[outputOffset + 3], fixedWeightedValues.w);
    }
}

/// SampleLogits
#pragma kernel SampleLogits

StructuredBuffer<float> sample_probabilities;
RWStructuredBuffer<int> sample_result;
uint sample_length;
float sample_random;

// YIKES!  Make this run in parallel!!!

[numthreads(1, 1, 1)]
void SampleLogits(uint3 id : SV_DispatchThreadID) {
    sample_result[0] = sample_length - 1;
    float cdf = 0.0f;
    for (uint i = 0; i < sample_length; i++) {
        cdf += sample_probabilities[i];
        if (sample_random < cdf) {
            sample_result[0] = i;
            break;
        }
    }
}

/// FindMax
#pragma kernel FindMaxIndex

StructuredBuffer<float> findmaxidx_values;
RWStructuredBuffer<int> findmaxidx_output;

uint findmaxidx_length;
groupshared int findmaxidx_groupMaxIdx;

[numthreads(kThreadsPerGroup, 1, 1)]
void FindMaxIndex(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint i = id.x;
    if (i >= findmaxidx_length) return;

    if (all(gid) == 0)
    {
        findmaxidx_groupMaxIdx = -1;
    }

    GroupMemoryBarrierWithGroupSync();

    float value = findmaxidx_values[i];
    
    // First find max in local group
    while (true)
    {
        int currentMaxIdx = findmaxidx_groupMaxIdx;
        float currentMax = currentMaxIdx > 0 ? findmaxidx_values[currentMaxIdx] : -1000.0f;
        if (value > currentMax)
        {
            int oldMax;
            InterlockedCompareExchange(findmaxidx_groupMaxIdx, currentMaxIdx, i, oldMax);
            if (oldMax == currentMax)
            {
                break;
            }
        }
        else
        {
            break;
        }
    }

    GroupMemoryBarrierWithGroupSync();

    // Then write the group max to global memory
    while (true)
    {
        int currentMaxIdx = findmaxidx_output[0];
        float currentMax = currentMaxIdx > 0 ? findmaxidx_values[currentMaxIdx] : -1000.0f;
        if (value > currentMax)
        {
            int oldMax;
            InterlockedCompareExchange(findmaxidx_output[0], currentMaxIdx, i, oldMax);
            if (oldMax == currentMax)
            {
                break;
            }
        }
        else
        {
            break;
        }
    }
}

/// FindMaxValue
#pragma kernel FindMaxValue

StructuredBuffer<float> findmaxval_input;
RWStructuredBuffer<int> findmaxval_output;
uint findmaxval_length;
uint findmaxval_offset;

groupshared int findmaxval_groupMaxVal;

[numthreads(kThreadsPerGroup, 1, 1)]
void FindMaxValue(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    if (gid.x == 0)
    {
        findmaxval_groupMaxVal = 0;
    }

    GroupMemoryBarrierWithGroupSync();

    uint i = id.x;
    if (i < findmaxval_length)
    {
        float sourceValue = findmaxval_input[findmaxval_offset + i];
        // HACK!!!!
        // InterlockedMax doesn't seem to work with a mix of positive and negative numbers on Vulkan.  Perhaps
        // because it is interpreting the value as either float or uint?  Not sure.  To fix that, we force the value
        // to be positive before taking the max, and we will convert it back when converting from fixed point.
        sourceValue += 256.0f;
        int valueFixed = (int)(sourceValue * kFixedPointScale);
        InterlockedMax(findmaxval_groupMaxVal, valueFixed);
    }

    GroupMemoryBarrierWithGroupSync();

    if (gid.x == 0)
    {
        InterlockedMax(findmaxval_output[0], findmaxval_groupMaxVal);
    }
}