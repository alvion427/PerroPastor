/*
.----. .----.  .--.  .----.    .-.   .-..-. .----..---. .----.   .--.  .-.   
| {}  \| {_   / {} \ | {}  }   |  `.'  || |{ {__ {_   _}| {}  } / {} \ | |   
|     /| {__ /  /\  \| .-. \   | |\ /| || |.-._} } | |  | .-. \/  /\  \| `--.
`----' `----'`-'  `-'`-' `-'   `-' ` `-'`-'`----'  `-'  `-' `-'`-'  `-'`----'

Don't judge this code for it's performance!  I have done almost no low level
optimization of this code due to the fact that that the attention code still
needs a near total rewrite.  It's easier to debug in its current state, but I
have already begun a rewrite the way it should be:
- Attention rewritten to compute small blocks of attention entirely within local
  memory, without writing interemediate values (ie attention weights) out to main
  memory.  (see https://arxiv.org/abs/2112.05682)
- Fused attention kernels.  Attention should only be two kernels, one for computing 
  attention and one for the divide at the end.  This replaces three, much slower kernels.

These changes will yield more performance than anything else I could do, and should be 
done before I do any low level optimizations.
*/

#pragma multi_compile QUANT_WEIGHT_32 QUANT_WEIGHT_16 QUANT_WEIGHT_Q5_1 QUANT_WEIGHT_Q8_0
//#define QUANT_WEIGHT_Q5_1 1
#pragma multi_compile QUANT_RUNTIME_32 QUANT_RUNTIME_16

#include "Common.cginc"

#if QUANT_WEIGHT_16
    #define WEIGHT_BLOCK_TYPE uint2
    #define STORE_WEIGHT StoreVec16f
    #define LOAD_WEIGHT LoadVec16f
    #define WEIGHT_IS_QUANTIZED 0
#elif QUANT_WEIGHT_Q5_1
    #define WEIGHT_BLOCK_VEC 8
    #define WEIGHT_BLOCK_TYPE Q5_1_Block
    #define ENCODE_WEIGHT Encode_Q5_1
    #define DECODE_WEIGHT Decode_Q5_1
    #define WEIGHT_IS_QUANTIZED 1
#elif QUANT_WEIGHT_Q8_0
    #define WEIGHT_BLOCK_VEC 8
    #define WEIGHT_BLOCK_TYPE Q8_0_Block
    #define ENCODE_WEIGHT Encode_Q8_0
    #define DECODE_WEIGHT Decode_Q8_0
    #define WEIGHT_IS_QUANTIZED 1
#else /* QUANT_WEIGHT_32 */
    #define WEIGHT_BLOCK_TYPE float4
    #define STORE_WEIGHT StoreVec32f
    #define LOAD_WEIGHT LoadVec32f
    #define WEIGHT_IS_QUANTIZED 0
#endif

#if QUANT_RUNTIME_16
    #define RUNTIME_TYPE_VEC uint2
    #define StoreRuntime StoreVec16f
    #define LoadRuntime LoadVec16f
#else  /* QUANT_RUNTIME_32 */
    #define RUNTIME_TYPE_VEC float4
    #define StoreRuntime StoreVec32f
    #define LoadRuntime LoadVec32f
#endif

/// Clear
#pragma kernel Clear

RWStructuredBuffer<float> clear_dest;
uint clear_length;

[numthreads(256, 1, 1)]
void Clear(uint3 id : SV_DispatchThreadID) {
    if (id.x >= clear_length) return;
    clear_dest[id.x] = 0;
}

/// Memcpy
#pragma kernel Memcpy

RWStructuredBuffer<RUNTIME_TYPE_VEC> copy_dest;
StructuredBuffer<float4> copy_source;
uint memcpy_veclen;
uint memcpy_source_offset;
uint memcpy_dest_offset;

[numthreads(256, 1, 1)]
void Memcpy(uint3 id : SV_DispatchThreadID)
{
    // Check if index is within bounds
    if (id.x < memcpy_veclen)
    {
        // TODO: Change the load to also use LoadRuntime() once we convert everything to float16
        float4 v = copy_source[id.x + memcpy_source_offset];
        copy_dest[id.x + memcpy_dest_offset] = StoreRuntime(v);
    }
}

/// ScaleBuffer
#pragma kernel ScaleBuffer

RWStructuredBuffer<float4> scalebuffer_buffer;
uint scalebuffer_veclen;
float scalebuffer_scale;

[numthreads(kThreadsPerGroup, 1, 1)]
void ScaleBuffer(uint3 id : SV_DispatchThreadID) {
    if (id.x >= scalebuffer_veclen) return;
    
    scalebuffer_buffer[id.x] = scalebuffer_buffer[id.x] * scalebuffer_scale;
}


/// Fixed to float
#pragma kernel FixedToFloat

RWStructuredBuffer<float4> fixedtofloat_dest;
StructuredBuffer<int4> fixedtofloat_source;
uint fixedtofloat_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void FixedToFloat(uint3 id : SV_DispatchThreadID) {
    if (id.x >= fixedtofloat_length) return;
    fixedtofloat_dest[id.x] = fixedtofloat_source[id.x] / kFixedPointScale;
}

#pragma kernel LoadEmbedding

StructuredBuffer<int> loadembedding_token;
StructuredBuffer<WEIGHT_BLOCK_TYPE> loadembedding_source;
RWStructuredBuffer<float4> loadembedding_dest;
uint loadembedding_blockCount;

[numthreads(kThreadsPerGroup, 1, 1)]
void LoadEmbedding(uint3 id : SV_DispatchThreadID) {
    if (id.x >= loadembedding_blockCount) return;

    int token = loadembedding_token[0];
    int sourceIndex = token * loadembedding_blockCount + id.x;

#if !WEIGHT_IS_QUANTIZED    
    loadembedding_dest[id.x] = LOAD_WEIGHT(loadembedding_source[sourceIndex]);
#else
    const WEIGHT_BLOCK_TYPE block = loadembedding_source[sourceIndex];

    int destOffset = id.x * WEIGHT_BLOCK_VEC;

    [unroll]
    for (uint valueIndex = 0; valueIndex < WEIGHT_BLOCK_VEC; ++valueIndex)
    {
        float4 v = DECODE_WEIGHT(block, valueIndex);
        loadembedding_dest[destOffset + valueIndex] = v;
    }
#endif
}

/// MatMul
#pragma kernel MatMul

StructuredBuffer<WEIGHT_BLOCK_TYPE> matmul_matrixW;
StructuredBuffer<float> matmul_vectorX;
RWStructuredBuffer<int> matmul_vectorOutFixed;
uint matmul_rows;
uint matmul_cols;
uint matmul_blocksPerRow;

#define kMatmulGroupRows 32
#define kMatmulGroupCols 1024

// A "superblock" is 32 columns. This is equal to one block for quantized formats, and 8 blocks of 4 values for
// non-quantized.
#define kMatmulGroupSuperBlocks (kMatmulGroupCols / 32) 

groupshared float matmul_vectorXCache[kMatmulGroupCols];
groupshared int matmul_groupOutput[kMatmulGroupRows];

// We need to write out one group output per group
//groupshared int matmul_groupOutput[kMatmulMaxRows / kMatmulGroupRows];

// So, dispatch group size is 32x32 for 1024 threads.
// Each group covers 1024 columns and 32 rows.
// x is row, and y is column (opposite of in graphics)
[numthreads(kMatmulGroupRows, kMatmulGroupSuperBlocks, 1)]
void MatMul(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID, uint3 groupId : SV_GroupID) {

    // Init group output
    if (gid.y == 0)
    {
        matmul_groupOutput[gid.x] = 0;
    }

    // Load the values of X we care about into groupshared memory
    uint gidFlat = gid.y * kMatmulGroupRows + gid.x;
    if (gidFlat < min(kMatmulGroupCols, matmul_cols))
    {
        matmul_vectorXCache[gidFlat] = matmul_vectorX[groupId.y * kMatmulGroupCols + gidFlat];
    }

    GroupMemoryBarrierWithGroupSync();

    // x is row, y is column
    const uint row = id.x;

    float sum = 0;
#if !WEIGHT_IS_QUANTIZED
    if (id.y * 8 < matmul_blocksPerRow)
    {
        [unroll]
        for (uint colVec = 0; colVec < 8; ++colVec)
        {
            float4 aVal = LOAD_WEIGHT(matmul_matrixW[row * matmul_blocksPerRow + id.y * 8 + colVec]);
            const uint inputIdx = (gid.y * 8 + colVec);
            float4 bVal = GetVector(matmul_vectorXCache, inputIdx);
            sum += dot(aVal, bVal);
        }
    }
#else
    if (id.y < matmul_blocksPerRow)
    {
        uint blockIdx = row * matmul_blocksPerRow + id.y;
        const WEIGHT_BLOCK_TYPE block = matmul_matrixW[blockIdx];
    
        [unroll]
        for (uint valueIndex = 0; valueIndex < WEIGHT_BLOCK_VEC; ++valueIndex)
        {
            float4 aVal = DECODE_WEIGHT(block, valueIndex);
            const uint inputIdx = (gid.y * 8) + valueIndex;
            float4 bVal = GetVector(matmul_vectorXCache, inputIdx);
            sum += dot(aVal, bVal);

        }
    }
#endif
    
    // Each thread accumulates the results of its superblock into the groupshared memory.  There is one value per row.
    const int sumFixed = (int)(sum * kFixedPointScale);
    InterlockedAdd(matmul_groupOutput[gid.x], sumFixed);

    GroupMemoryBarrierWithGroupSync();

    // One thread per row, per group accumulates the results in the final result
    if (gid.y == 0)
    {
        InterlockedAdd(matmul_vectorOutFixed[id.x], matmul_groupOutput[gid.x]);
    }
}

/// Accumulate
#pragma kernel Accumulate

RWStructuredBuffer<float4> accumulate_A;
StructuredBuffer<float4> accumulate_B;
uint accumulate_veclen;

[numthreads(kThreadsPerGroup, 1, 1)]
void Accumulate(uint3 id : SV_DispatchThreadID) {
    if (id.x >= accumulate_veclen) return;
    accumulate_A[id.x] += accumulate_B[id.x];
}

/// RMSNorm
#pragma kernel RMSNorm

StructuredBuffer<float4> rmsnorm_In; // Input values
StructuredBuffer<WEIGHT_BLOCK_TYPE> rmsnorm_Weight; // Weights
RWStructuredBuffer<float4> rmsnorm_Out; // Output values
uint rmsnorm_vecLen;
float rmsnorm_length;

groupshared uint rmsnorm_ss_integer;
groupshared uint rmsnorm_ss_fraction;

[numthreads(1024, 1, 1)]
void RMSNorm(uint3 id : SV_DispatchThreadID) {

    if (id.x == 0)
    {
        rmsnorm_ss_integer = 0;
        rmsnorm_ss_fraction = 0;
    }

    GroupMemoryBarrierWithGroupSync();

    uint j = id.x;

    if (j < rmsnorm_vecLen)
    {
        // RMSNorm denominator can grow very large (in the hundreds of thousands) or be very small in which case
        // it needs to be precise, so we can't use our normal 16/16 fixed point.  Instead we use two floats for 32/32.
        const float val = dot(rmsnorm_In[j], rmsnorm_In[j]);
        const uint valInt = (uint)val;
        const float fraction = val - (float)valInt;
        InterlockedAdd(rmsnorm_ss_integer, valInt);
        InterlockedAdd(rmsnorm_ss_fraction, (uint)round(fraction * kFixedPointScale));
    }
        
    GroupMemoryBarrierWithGroupSync();

    if (j < rmsnorm_vecLen)
    {
        const float kEpsilon = 1e-5f;
        float ss = (float)rmsnorm_ss_integer + (float)rmsnorm_ss_fraction / kFixedPointScale;
        ss /= rmsnorm_length;
        ss += kEpsilon;
        ss = 1.0f / sqrt(ss);

        // Normalize and scale
#if !WEIGHT_IS_QUANTIZED
        float4 weights = LOAD_WEIGHT(rmsnorm_Weight[j]);
        rmsnorm_Out[j] = weights * (ss * rmsnorm_In[j]);
#else
        uint b = j / WEIGHT_BLOCK_VEC;
        const WEIGHT_BLOCK_TYPE block = rmsnorm_Weight[b];
        const uint valueIndex = j % WEIGHT_BLOCK_VEC;
        float4 weights = DECODE_WEIGHT(block, valueIndex);
        rmsnorm_Out[j] = weights * (ss * rmsnorm_In[j]);
#endif
    }
}

/// Rope
#pragma kernel Rope

RWStructuredBuffer<float2> rope_q; // Query vectors
RWStructuredBuffer<float2> rope_k; // Key vectors
uint rope_head_size;
uint rope_pos;
uint rope_half_dim_vec;
uint rope_kv_half_dim_vec;

[numthreads(kThreadsPerGroup, 1, 1)]
void Rope(uint3 id : SV_DispatchThreadID) {
    uint idx = id.x;
    if (idx >= rope_half_dim_vec) return;

    uint i = idx * 2;
    uint head_idx = i % rope_head_size;

    float freq = 1.0f / pow(10000.0f, head_idx / (float)rope_head_size);
    float val = rope_pos * freq;
    float fcr = cos(val);
    float fci = sin(val);
    float2 fc = float2(fcr, fci);

    float q0 = rope_q[idx].x;
    float q1 = rope_q[idx].y;
    rope_q[idx].x = q0 * fc.x - q1 * fc.y;
    rope_q[idx].y = q0 * fc.y + q1 * fc.x;

    if (idx < rope_kv_half_dim_vec)
    {
        float k0 = rope_k[idx].x;
        float k1 = rope_k[idx].y;
        rope_k[idx].x = k0 * fc.x - k1 * fc.y;
        rope_k[idx].y = k0 * fc.y + k1 * fc.x;
    }
}

#pragma kernel SoftmaxExp
#pragma kernel SoftmaxDivide

StructuredBuffer<float> softmax_input;
RWStructuredBuffer<float> softmax_output;
StructuredBuffer<int> softmax_max_fixed;
RWStructuredBuffer<int> softmax_sum_fixed;
uint softmax_length;
uint softmax_numBatches;
uint softmax_offset;

#define SOFTMAX_STRIDE 8

groupshared int softmax_groupSumFixed;
groupshared float softmax_maxValue;

[numthreads(256, 1, 1)]
void SoftmaxExp(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    if (gid.x == 0)
    {
        softmax_groupSumFixed = 0;
        softmax_maxValue = (softmax_max_fixed[0] / kFixedPointScale);

        // HACK!!! See comment in FindMaxValue
        softmax_maxValue -= 256.0f;
    }

    GroupMemoryBarrierWithGroupSync();
    
    if (id.x < softmax_numBatches)
    {
        uint start = id.x * SOFTMAX_STRIDE;
        uint end = min(start + SOFTMAX_STRIDE, softmax_length); 
        float totalExp = 0;
        for (uint j = start; j < end; ++j)
        {
            float xExp = exp(softmax_input[softmax_offset + j] - softmax_maxValue);
            softmax_output[j] = xExp;
            totalExp += xExp;
        }

        int xExpFixed = (int)(totalExp * kFixedPointScale);
        InterlockedAdd(softmax_groupSumFixed, xExpFixed);
    }

    GroupMemoryBarrierWithGroupSync();

    if (gid.x == 0)
    {
        InterlockedAdd(softmax_sum_fixed[0], softmax_groupSumFixed);
    }
}

[numthreads(kThreadsPerGroup, 1, 1)]
void SoftmaxDivide(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint i = id.x;
    if (i >= softmax_length) return;

    float sum = softmax_sum_fixed[0] / kFixedPointScale;
    softmax_output[softmax_offset + i] = softmax_input[i] / sum;
}


/// Compute attention
#pragma kernel ComputeAttention

StructuredBuffer<float4> compute_attention_q; // Query vectors
StructuredBuffer<RUNTIME_TYPE_VEC> compute_attention_k; // Key cache
RWStructuredBuffer<float> compute_attention_att; // Attention scores

uint compute_attention_head; // Which head we are processing
uint compute_attention_head_size_vec; // Size of each head
uint compute_attention_kv_div; // How many heads share a key/value vector
uint compute_attention_pos; // Current position in the sequence
uint compute_attention_dim_vec; // Dimension of embeddings
uint compute_attention_seq_len; // Length of the sequence

float compute_attention_head_size_inv_sqrt; // 1.0 / Square root of the head size

[numthreads(kThreadsPerGroup, 1, 1)]
void ComputeAttention(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint t = id.x;
    uint att_offset = compute_attention_head * compute_attention_seq_len;
    
    if (t > compute_attention_pos) {
        return;
    }

    uint headOffset = compute_attention_head * compute_attention_head_size_vec;
    uint k_offset = (t * compute_attention_dim_vec + headOffset) / compute_attention_kv_div;

    // Compute attention score as dot product of q and k
    float score = 0;
    for (uint i = 0; i < compute_attention_head_size_vec; i++)
    {
        float4 q = compute_attention_q[headOffset + i];
        float4 k = LoadRuntime(compute_attention_k[k_offset + i]);
        score += dot(q, k);
    }
    
    score *= compute_attention_head_size_inv_sqrt;
    compute_attention_att[att_offset + t] = score;
}

/// Silu
#pragma kernel Silu

RWStructuredBuffer<float> silu_InOut;
uint silu_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void Silu(uint3 id : SV_DispatchThreadID) {
    if (id.x >= silu_length) return;
    float x = silu_InOut[id.x];
    silu_InOut[id.x] = x * (1.0f / (1.0f + exp(-x)));
}

/// Elementwise Multiply
#pragma kernel Multiply

RWStructuredBuffer<float> multiply_A;
StructuredBuffer<float> multiply_B;
uint multiply_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void Multiply(uint3 id : SV_DispatchThreadID) {
    if (id.x >= multiply_length) return;
    multiply_A[id.x] *= multiply_B[id.x];
}

/// WeightedSum
#pragma kernel WeightedSum

// Global buffers and variables for Weighted Sum computation
StructuredBuffer<RUNTIME_TYPE_VEC> weightedsum_values; // Value vectors
StructuredBuffer<float> weightedsum_attention; // Attention weights
RWStructuredBuffer<int> weightedsum_out; // Output buffer to hold weighted sum

uint weightedsum_offset_vec; // Offset into value buffer
uint weightedsum_attention_offset; // Offset into attention buffer
uint weightedsum_head_size_vec;
uint weightedsum_pos;
uint weightedsum_dim_vec;

[numthreads(kThreadsPerGroup, 1, 1)]
void WeightedSum(uint3 id : SV_DispatchThreadID) {
    uint t = id.x;
    if (t > weightedsum_pos) return;

    uint valuesOffset = weightedsum_offset_vec + t * weightedsum_dim_vec;

    // accumulate the weighted value into output
    float a = weightedsum_attention[weightedsum_attention_offset + t];
    for (uint i = 0; i < weightedsum_head_size_vec; i++) {
        float4 values = LoadRuntime(weightedsum_values[valuesOffset + i]);
        float4 weightedValues = a * values;
        int4 fixedWeightedValues = (int4)(weightedValues * kFixedPointScale);

        int outputOffset = (weightedsum_offset_vec + i) * 4;
        InterlockedAdd(weightedsum_out[outputOffset + 0], fixedWeightedValues.x);
        InterlockedAdd(weightedsum_out[outputOffset + 1], fixedWeightedValues.y);
        InterlockedAdd(weightedsum_out[outputOffset + 2], fixedWeightedValues.z);
        InterlockedAdd(weightedsum_out[outputOffset + 3], fixedWeightedValues.w);
    }
}

/// SampleLogits
#pragma kernel SampleLogits

StructuredBuffer<float> sample_probabilities;
RWStructuredBuffer<int> sample_result;
uint sample_length;
float sample_random;

// YIKES!  Make this run in parallel!!!

[numthreads(1, 1, 1)]
void SampleLogits(uint3 id : SV_DispatchThreadID) {
    sample_result[0] = sample_length - 1;
    float cdf = 0.0f;
    for (uint i = 0; i < sample_length; i++) {
        cdf += sample_probabilities[i];
        if (sample_random < cdf) {
            sample_result[0] = i;
            break;
        }
    }
}

/// FindMax
#pragma kernel FindMaxIndex

StructuredBuffer<float> findmaxidx_values;
RWStructuredBuffer<int> findmaxidx_output;

uint findmaxidx_length;
groupshared int findmaxidx_groupMaxIdx;

[numthreads(kThreadsPerGroup, 1, 1)]
void FindMaxIndex(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint i = id.x;
    if (i >= findmaxidx_length) return;

    if (all(gid) == 0)
    {
        findmaxidx_groupMaxIdx = -1;
    }

    GroupMemoryBarrierWithGroupSync();

    float value = findmaxidx_values[i];
    
    // First find max in local group
    while (true)
    {
        int currentMaxIdx = findmaxidx_groupMaxIdx;
        float currentMax = currentMaxIdx > 0 ? findmaxidx_values[currentMaxIdx] : -1000.0f;
        if (value > currentMax)
        {
            int oldMax;
            InterlockedCompareExchange(findmaxidx_groupMaxIdx, currentMaxIdx, i, oldMax);
            if (oldMax == currentMax)
            {
                break;
            }
        }
        else
        {
            break;
        }
    }

    GroupMemoryBarrierWithGroupSync();

    // Then write the group max to global memory
    while (true)
    {
        int currentMaxIdx = findmaxidx_output[0];
        float currentMax = currentMaxIdx > 0 ? findmaxidx_values[currentMaxIdx] : -1000.0f;
        if (value > currentMax)
        {
            int oldMax;
            InterlockedCompareExchange(findmaxidx_output[0], currentMaxIdx, i, oldMax);
            if (oldMax == currentMax)
            {
                break;
            }
        }
        else
        {
            break;
        }
    }
}

/// FindMaxValue
#pragma kernel FindMaxValue

StructuredBuffer<float> findmaxval_input;
RWStructuredBuffer<int> findmaxval_output;
uint findmaxval_length;
uint findmaxval_offset;

groupshared int findmaxval_groupMaxVal;

[numthreads(kThreadsPerGroup, 1, 1)]
void FindMaxValue(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    if (gid.x == 0)
    {
        findmaxval_groupMaxVal = 0;
    }

    GroupMemoryBarrierWithGroupSync();

    uint i = id.x;
    if (i < findmaxval_length)
    {
        float sourceValue = findmaxval_input[findmaxval_offset + i];
        // HACK!!!!
        // InterlockedMax doesn't seem to work with a mix of positive and negative numbers on Vulkan.  Perhaps
        // because it is interpreting the value as either float or uint?  Not sure.  To fix that, we force the value
        // to be positive before taking the max, and we will convert it back when converting from fixed point.
        sourceValue += 256.0f;
        int valueFixed = (int)(sourceValue * kFixedPointScale);
        InterlockedMax(findmaxval_groupMaxVal, valueFixed);
    }

    GroupMemoryBarrierWithGroupSync();

    if (gid.x == 0)
    {
        InterlockedMax(findmaxval_output[0], findmaxval_groupMaxVal);
    }
}