#pragma multi_compile QUANT_WEIGHT_32 QUANT_WEIGHT_16
#pragma multi_compile QUANT_RUNTIME_32 QUANT_RUNTIME_16

#include "Common.cginc"

#if QUANT_WEIGHT_16
    #define WEIGHT_TYPE_VEC uint2
    #define StoreWeights StoreVec16f
    #define LoadWeights LoadVec16f
#else /* QUANT_WEIGHT_32 */
    #define WEIGHT_TYPE_VEC float4
    #define StoreWeights StoreVec32f
    #define LoadWeights LoadVec32f
#endif

#if QUANT_RUNTIME_16
    #define RUNTIME_TYPE_VEC uint2
    #define StoreRuntime StoreVec16f
    #define LoadRuntime LoadVec16f
#else  /* QUANT_RUNTIME_32 */
    #define RUNTIME_TYPE_VEC float4
    #define StoreRuntime StoreVec32f
    #define LoadRuntime LoadVec32f
#endif

/// Clear
#pragma kernel Clear

RWStructuredBuffer<float> clear_dest;
uint clear_length;

[numthreads(256, 1, 1)]
void Clear(uint3 id : SV_DispatchThreadID) {
    if (id.x >= clear_length) return;
    clear_dest[id.x] = 0;
}

/// Memcpy
#pragma kernel Memcpy

RWStructuredBuffer<RUNTIME_TYPE_VEC> copy_dest;
StructuredBuffer<float4> copy_source;
uint memcpy_veclen;
uint memcpy_source_offset;
uint memcpy_dest_offset;

[numthreads(256, 1, 1)]
void Memcpy(uint3 id : SV_DispatchThreadID)
{
    // Check if index is within bounds
    if (id.x < memcpy_veclen)
    {
        // TODO: Change the load to also use LoadRuntime() once we convert everything to float16
        float4 v = copy_source[id.x + memcpy_source_offset];
        copy_dest[id.x + memcpy_dest_offset] = StoreRuntime(v);
    }
}

/// ScaleBuffer
#pragma kernel ScaleBuffer

RWStructuredBuffer<float4> scalebuffer_buffer;
uint scalebuffer_veclen;
float scalebuffer_scale;

[numthreads(kThreadsPerGroup, 1, 1)]
void ScaleBuffer(uint3 id : SV_DispatchThreadID) {
    if (id.x >= scalebuffer_veclen) return;
    
    scalebuffer_buffer[id.x] = scalebuffer_buffer[id.x] * scalebuffer_scale;
}


/// Fixed to float
#pragma kernel FixedToFloat

RWStructuredBuffer<float4> fixedtofloat_dest;
StructuredBuffer<int4> fixedtofloat_source;
uint fixedtofloat_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void FixedToFloat(uint3 id : SV_DispatchThreadID) {
    if (id.x >= fixedtofloat_length) return;
    fixedtofloat_dest[id.x] = fixedtofloat_source[id.x] / kFixedPointScale;
}

#pragma kernel LoadEmbedding

StructuredBuffer<int> loadembedding_token;
StructuredBuffer<WEIGHT_TYPE_VEC> loadembedding_source;
RWStructuredBuffer<float4> loadembedding_dest;
uint loadembedding_veclen;

[numthreads(kThreadsPerGroup, 1, 1)]
void LoadEmbedding(uint3 id : SV_DispatchThreadID) {
    if (id.x >= loadembedding_veclen) return;
    int token = loadembedding_token[0];
    int sourceOffset = token * loadembedding_veclen;
    loadembedding_dest[id.x] = LoadWeights(loadembedding_source[sourceOffset + id.x]);
}

/// MatMul
#pragma kernel MatMul

StructuredBuffer<WEIGHT_TYPE_VEC> matmul_matrixW;
StructuredBuffer<float4> matmul_vectorX;
RWStructuredBuffer<float> matmul_vectorOut;
uint matmul_rows;
uint matmul_cols_vec;

[numthreads(kThreadsPerGroup, 1, 1)]
void MatMul(uint3 id : SV_DispatchThreadID) {
    uint row = id.x;
    if (row >= matmul_rows) return;
    
    float sum = 0;
    for (uint col = 0; col < matmul_cols_vec; col++) {
        float4 a = LoadWeights(matmul_matrixW[id.x * matmul_cols_vec + col]);
        float4 b = matmul_vectorX[col];
        sum += dot(a, b);
    }
    matmul_vectorOut[row] = sum;
}

#pragma kernel MatMulTex

Texture2D<float> matmultex_matrixW;
//sampler2D sampler_matmultex_matrixW;

StructuredBuffer<float> matmultex_vectorX;
RWStructuredBuffer<float> matmultex_vectorOut;
uint matmultex_rows;
uint matmultex_cols;

[numthreads(kThreadsPerGroup, 1, 1)]
void MatMulTex(uint3 id : SV_DispatchThreadID) {
    uint row = id.x;
    if (row >= matmultex_rows) return;

    float sum = 0;
    for (uint col = 0; col < matmultex_cols; col++) {
        const float matrixWValue0 = matmultex_matrixW[uint2(col, row)];
        sum += matrixWValue0 * matmultex_vectorX[col];
    }

    matmultex_vectorOut[row] = sum;
}

/// Accumulate
#pragma kernel Accumulate

RWStructuredBuffer<float4> accumulate_A;
StructuredBuffer<float4> accumulate_B;
uint accumulate_veclen;

[numthreads(kThreadsPerGroup, 1, 1)]
void Accumulate(uint3 id : SV_DispatchThreadID) {
    if (id.x >= accumulate_veclen) return;
    accumulate_A[id.x] += accumulate_B[id.x];
}

/// RMSNorm
#pragma kernel RMSNorm

StructuredBuffer<float4> rmsnorm_In; // Input values
StructuredBuffer<WEIGHT_TYPE_VEC> rmsnorm_Weight; // Weights
RWStructuredBuffer<float4> rmsnorm_Out; // Output values
uint rmsnorm_veclen; // Length of the input and output vectors
float rmsnorm_length;

[numthreads(1, 1, 1)]
void RMSNorm(uint3 id : SV_DispatchThreadID) {
    const float kEpsilon = 1e-5f;
    uint i = id.x;
    if (i >= rmsnorm_veclen) return;

    uint j;
    
    // Calculate sum of squares
    float ss = 0.0f;
    for (j = 0; j < rmsnorm_veclen; j++) {
        ss += dot(rmsnorm_In[j], rmsnorm_In[j]);
    }
    ss /= rmsnorm_length;
    ss += kEpsilon;
    ss = 1.0f / sqrt(ss);

    // Normalize and scale
    for (j = 0; j < rmsnorm_veclen; ++j)
    {
        float4 weights = LoadWeights(rmsnorm_Weight[j]);
        rmsnorm_Out[j] = weights * (ss * rmsnorm_In[j]);
    }
}

/// Rope
#pragma kernel Rope

RWStructuredBuffer<float2> rope_q; // Query vectors
RWStructuredBuffer<float2> rope_k; // Key vectors
uint rope_head_size;
uint rope_pos;
uint rope_length; // Total length of the vectors

[numthreads(kThreadsPerGroup, 1, 1)]
void Rope(uint3 id : SV_DispatchThreadID) {
    uint idx = id.x;
    if (idx >= rope_length) return;

    uint i = idx * 2;
    uint head_dim = i % rope_head_size;

    float freq = 1.0f / pow(10000.0f, head_dim / (float)rope_head_size);
    float val = rope_pos * freq;
    float fcr = cos(val);
    float fci = sin(val);
    float2 fc = float2(fcr, fci);
    
    float q0 = rope_q[idx].x;
    float q1 = rope_q[idx].y;
    float k0 = rope_k[idx].x;
    float k1 = rope_k[idx].y;

    rope_q[idx].x = q0 * fc.x - q1 * fc.y;
    rope_q[idx].y = q0 * fc.y + q1 * fc.x;
    rope_k[idx].x = k0 * fc.x - k1 * fc.y;
    rope_k[idx].y = k0 * fc.y + k1 * fc.x;
}

#pragma kernel SoftmaxExp
#pragma kernel SoftmaxDivide

StructuredBuffer<float> softmax_input;
RWStructuredBuffer<float> softmax_output;
StructuredBuffer<int> softmax_max_fixed;
RWStructuredBuffer<int> softmax_sum_fixed;
uint softmax_length;
uint softmax_offset;

groupshared int softmax_groupSumFixed;

[numthreads(kThreadsPerGroup, 1, 1)]
void SoftmaxExp(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint i = id.x;
    if (i >= softmax_length) return;

    if (all(gid) == 0)
    {
        softmax_groupSumFixed = 0;
    }

    GroupMemoryBarrierWithGroupSync();

    float maxValue = softmax_max_fixed[0] / kFixedPointScale;
    float xExp = exp(softmax_input[softmax_offset + i] - maxValue);
    softmax_output[softmax_offset + i] = xExp;

    int xExpFixed = (int)(xExp * kFixedPointScale);
    InterlockedAdd(softmax_groupSumFixed, xExpFixed);

    GroupMemoryBarrierWithGroupSync();

    InterlockedAdd(softmax_sum_fixed[0], (int)(xExp * kFixedPointScale));
}

[numthreads(kThreadsPerGroup, 1, 1)]
void SoftmaxDivide(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint i = id.x;
    if (i >= softmax_length) return;

    float sum = softmax_sum_fixed[0] / kFixedPointScale;
    softmax_output[softmax_offset + i] = softmax_input[softmax_offset + i] / sum;
}


/// Compute attention
#pragma kernel ComputeAttention

StructuredBuffer<float4> compute_attention_q; // Query vectors
StructuredBuffer<RUNTIME_TYPE_VEC> compute_attention_k; // Key cache
//StructuredBuffer<uint2> compute_attention_k; // Key cache
RWStructuredBuffer<float> compute_attention_att; // Attention scores

uint compute_attention_head; // Which head we are processing
uint compute_attention_head_size_vec; // Size of each head
uint compute_attention_pos; // Current position in the sequence
uint compute_attention_dim_vec; // Dimension of embeddings
uint compute_attention_seq_len; // Length of the sequence

float compute_attention_head_size_inv_sqrt; // 1.0 / Square root of the head size

[numthreads(kThreadsPerGroup, 1, 1)]
void ComputeAttention(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint t = id.x;
    uint att_offset = compute_attention_head * compute_attention_seq_len;
    
    if (t > compute_attention_pos) {
        return;
    }

    uint headOffset = compute_attention_head * compute_attention_head_size_vec;
    uint k_offset = t * compute_attention_dim_vec + headOffset;

    // Compute attention score as dot product of q and k
    float score = 0;
    for (uint i = 0; i < compute_attention_head_size_vec; i++)
    {
        float4 q = compute_attention_q[headOffset + i];
        float4 k = LoadRuntime(compute_attention_k[k_offset + i]);
        score += dot(q, k);
    }
    
    score *= compute_attention_head_size_inv_sqrt;
    compute_attention_att[att_offset + t] = score;
}

/// Silu
#pragma kernel Silu

RWStructuredBuffer<float> silu_InOut;
uint silu_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void Silu(uint3 id : SV_DispatchThreadID) {
    if (id.x >= silu_length) return;
    float x = silu_InOut[id.x];
    silu_InOut[id.x] = x * (1.0f / (1.0f + exp(-x)));
}

/// Elementwise Multiply
#pragma kernel Multiply

RWStructuredBuffer<float> multiply_A;
StructuredBuffer<float> multiply_B;
uint multiply_length;

[numthreads(kThreadsPerGroup, 1, 1)]
void Multiply(uint3 id : SV_DispatchThreadID) {
    if (id.x >= multiply_length) return;
    multiply_A[id.x] *= multiply_B[id.x];
}

/// WeightedSum
#pragma kernel WeightedSum

// Global buffers and variables for Weighted Sum computation
StructuredBuffer<RUNTIME_TYPE_VEC> weightedsum_values; // Value vectors
StructuredBuffer<float> weightedsum_attention; // Attention weights
RWStructuredBuffer<int> weightedsum_out; // Output buffer to hold weighted sum

uint weightedsum_offset_vec; // Offset into value buffer
uint weightedsum_attention_offset; // Offset into attention buffer
uint weightedsum_head_size_vec;
uint weightedsum_pos;
uint weightedsum_dim_vec;

[numthreads(kThreadsPerGroup, 1, 1)]
void WeightedSum(uint3 id : SV_DispatchThreadID) {
    uint t = id.x;
    if (t > weightedsum_pos) return;

    uint valuesOffset = weightedsum_offset_vec + t * weightedsum_dim_vec;

    // accumulate the weighted value into output
    float a = weightedsum_attention[weightedsum_attention_offset + t];
    for (uint i = 0; i < weightedsum_head_size_vec; i++) {
        float4 values = LoadRuntime(weightedsum_values[valuesOffset + i]);
        float4 weightedValues = a * values;
        int4 fixedWeightedValues = (int4)(weightedValues * kFixedPointScale);

        int outputOffset = (weightedsum_offset_vec + i) * 4;
        InterlockedAdd(weightedsum_out[outputOffset + 0], fixedWeightedValues.x);
        InterlockedAdd(weightedsum_out[outputOffset + 1], fixedWeightedValues.y);
        InterlockedAdd(weightedsum_out[outputOffset + 2], fixedWeightedValues.z);
        InterlockedAdd(weightedsum_out[outputOffset + 3], fixedWeightedValues.w);
    }
}

/// SampleLogits
#pragma kernel SampleLogits

StructuredBuffer<float> sample_probabilities;
RWStructuredBuffer<int> sample_result;
uint sample_length;
float sample_random;

// YIKES!  Make this run in parallel!!!

[numthreads(1, 1, 1)]
void SampleLogits(uint3 id : SV_DispatchThreadID) {
    sample_result[0] = sample_length - 1;
    float cdf = 0.0f;
    for (uint i = 0; i < sample_length; i++) {
        cdf += sample_probabilities[i];
        if (sample_random < cdf) {
            sample_result[0] = i;
            break;
        }
    }
}

/// FindMax
#pragma kernel FindMaxIndex

StructuredBuffer<float> findmaxidx_values;
RWStructuredBuffer<int> findmaxidx_output;

uint findmaxidx_length;
groupshared int findmaxidx_groupMaxIdx;

[numthreads(kThreadsPerGroup, 1, 1)]
void FindMaxIndex(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    uint i = id.x;
    if (i >= findmaxidx_length) return;

    if (all(gid) == 0)
    {
        findmaxidx_groupMaxIdx = -1;
    }

    GroupMemoryBarrierWithGroupSync();

    float value = findmaxidx_values[i];
    
    // First find max in local group
    while (true)
    {
        int currentMaxIdx = findmaxidx_groupMaxIdx;
        float currentMax = currentMaxIdx > 0 ? findmaxidx_values[currentMaxIdx] : -1000.0f;
        if (value > currentMax)
        {
            int oldMax;
            InterlockedCompareExchange(findmaxidx_groupMaxIdx, currentMaxIdx, i, oldMax);
            if (oldMax == currentMax)
            {
                break;
            }
        }
        else
        {
            break;
        }
    }

    GroupMemoryBarrierWithGroupSync();

    // Then write the group max to global memory
    while (true)
    {
        int currentMaxIdx = findmaxidx_output[0];
        float currentMax = currentMaxIdx > 0 ? findmaxidx_values[currentMaxIdx] : -1000.0f;
        if (value > currentMax)
        {
            int oldMax;
            InterlockedCompareExchange(findmaxidx_output[0], currentMaxIdx, i, oldMax);
            if (oldMax == currentMax)
            {
                break;
            }
        }
        else
        {
            break;
        }
    }
}

/// FindMaxValue
#pragma kernel FindMaxValue

StructuredBuffer<float> findmaxval_input;
RWStructuredBuffer<int> findmaxval_output;
uint findmaxval_length;
uint findmaxval_offset;

groupshared int findmaxval_groupMaxVal;

[numthreads(kThreadsPerGroup, 1, 1)]
void FindMaxValue(uint3 id : SV_DispatchThreadID, uint3 gid : SV_GroupThreadID) {
    if (all(gid) == 0)
    {
        findmaxval_groupMaxVal = -1000;
    }

    GroupMemoryBarrierWithGroupSync();

    uint i = id.x;
    if (i < findmaxval_length)
    {
        int valueFixed = (int)(findmaxval_input[findmaxval_offset + i] * kFixedPointScale);
        InterlockedMax(findmaxval_groupMaxVal, valueFixed);
    }

    GroupMemoryBarrierWithGroupSync();

    if (all(gid) == 0)
    {
        int groupMax = findmaxval_groupMaxVal;
        InterlockedMax(findmaxval_output[0], groupMax);
    }
}